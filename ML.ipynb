{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Phase\n",
    "\n",
    "### Author: Solomon Stevens\n",
    "### Date: July 28th, 2024\n",
    "\n",
    "Determine the best machine learning model for our MLB lack-of-run-support data set\n",
    "* Normalize all independant variables\n",
    "* Make the output boolean numeric (1 or 0)\n",
    "* Create and train multiple machine learning models\n",
    "* Test each model and log its performance\n",
    "* Compare and determine which is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "#-> For model creation\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#-> For scoring\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "\n",
    "print('Imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  last_name, first_name  year  home_run  k_percent  barrel_batted_rate  \\\n",
      "0          Adleman, Tim  2017        29       20.3                 7.9   \n",
      "1      Alcantara, Sandy  2019        23       18.0                 6.5   \n",
      "2      Alcantara, Sandy  2021        21       24.0                 6.1   \n",
      "3      Alcantara, Sandy  2022        16       23.4                 5.3   \n",
      "4      Alcantara, Sandy  2023        22       19.8                 7.0   \n",
      "\n",
      "   hard_hit_percent  lack_of_run_support  p_unearned_run  \n",
      "0              32.7                False               4  \n",
      "1              34.3                 True               9  \n",
      "2              39.4                 True              12  \n",
      "3              38.5                False               9  \n",
      "4              40.6                False               6  \n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('stats_EDA.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate a data frame to work with\n",
    "* We don't need factors such as the player's name or what year they pitched as independent variables.\n",
    "* We also need to noralize each variable to avoid unnecessary bias.\n",
    "  * Normalize with Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lack_of_run_support  home_run  p_unearned_run  k_percent  \\\n",
      "0                False        29               4       20.3   \n",
      "1                 True        23               9       18.0   \n",
      "2                 True        21              12       24.0   \n",
      "3                False        16               9       23.4   \n",
      "4                False        22               6       19.8   \n",
      "\n",
      "   barrel_batted_rate  hard_hit_percent  \n",
      "0                 7.9              32.7  \n",
      "1                 6.5              34.3  \n",
      "2                 6.1              39.4  \n",
      "3                 5.3              38.5  \n",
      "4                 7.0              40.6  \n"
     ]
    }
   ],
   "source": [
    "# Grab the columns we want\n",
    "#-> Include the dependent variable first for visual purposes\n",
    "\n",
    "df_ml = df[['lack_of_run_support', 'home_run', 'p_unearned_run', 'k_percent', 'barrel_batted_rate', 'hard_hit_percent']]\n",
    "\n",
    "print(df_ml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lack_of_run_support  home_run  p_unearned_run  k_percent  \\\n",
      "0                False  0.642857        0.173913   0.322476   \n",
      "1                 True  0.500000        0.391304   0.247557   \n",
      "2                 True  0.452381        0.521739   0.442997   \n",
      "3                False  0.333333        0.391304   0.423453   \n",
      "4                False  0.476190        0.260870   0.306189   \n",
      "\n",
      "   barrel_batted_rate  hard_hit_percent  \n",
      "0            0.457364          0.357430  \n",
      "1            0.348837          0.421687  \n",
      "2            0.317829          0.626506  \n",
      "3            0.255814          0.590361  \n",
      "4            0.387597          0.674699  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1119876379.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.home_run = (df_ml.home_run - df_ml.home_run.min()) / (df_ml.home_run.max() - df_ml.home_run.min())\n",
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1119876379.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.p_unearned_run = (df_ml.p_unearned_run - df_ml.p_unearned_run.min()) / (df_ml.p_unearned_run.max() - df_ml.p_unearned_run.min())\n",
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1119876379.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.k_percent = (df_ml.k_percent - df_ml.k_percent.min()) / (df_ml.k_percent.max() - df_ml.k_percent.min())\n",
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1119876379.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.barrel_batted_rate = (df_ml.barrel_batted_rate - df_ml.barrel_batted_rate.min()) / (df_ml.barrel_batted_rate.max() - df_ml.barrel_batted_rate.min())\n",
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1119876379.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.hard_hit_percent = (df_ml.hard_hit_percent - df_ml.hard_hit_percent.min()) / (df_ml.hard_hit_percent.max() - df_ml.hard_hit_percent.min())\n"
     ]
    }
   ],
   "source": [
    "# Normalize all the numeric columns\n",
    "#-> Home Runs Allowed\n",
    "df_ml.home_run = (df_ml.home_run - df_ml.home_run.min()) / (df_ml.home_run.max() - df_ml.home_run.min())\n",
    "\n",
    "#-> Unearned Runs Allowed\n",
    "df_ml.p_unearned_run = (df_ml.p_unearned_run - df_ml.p_unearned_run.min()) / (df_ml.p_unearned_run.max() - df_ml.p_unearned_run.min())\n",
    "\n",
    "#-> Strikeout Percent\n",
    "df_ml.k_percent = (df_ml.k_percent - df_ml.k_percent.min()) / (df_ml.k_percent.max() - df_ml.k_percent.min())\n",
    "\n",
    "#-> Barrel Batted Rate\n",
    "df_ml.barrel_batted_rate = (df_ml.barrel_batted_rate - df_ml.barrel_batted_rate.min()) / (df_ml.barrel_batted_rate.max() - df_ml.barrel_batted_rate.min())\n",
    "\n",
    "#-> Hard Hit Percent\n",
    "df_ml.hard_hit_percent = (df_ml.hard_hit_percent - df_ml.hard_hit_percent.min()) / (df_ml.hard_hit_percent.max() - df_ml.hard_hit_percent.min())\n",
    "\n",
    "\n",
    "print(df_ml.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\St0bs\\AppData\\Local\\Temp\\ipykernel_14836\\1298261037.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ml.lack_of_run_support = df_ml.lack_of_run_support.apply(lambda x: 1 if x==True else 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lack_of_run_support  home_run  p_unearned_run  k_percent  \\\n",
      "0                    0  0.642857        0.173913   0.322476   \n",
      "1                    1  0.500000        0.391304   0.247557   \n",
      "2                    1  0.452381        0.521739   0.442997   \n",
      "3                    0  0.333333        0.391304   0.423453   \n",
      "4                    0  0.476190        0.260870   0.306189   \n",
      "\n",
      "   barrel_batted_rate  hard_hit_percent  \n",
      "0            0.457364          0.357430  \n",
      "1            0.348837          0.421687  \n",
      "2            0.317829          0.626506  \n",
      "3            0.255814          0.590361  \n",
      "4            0.387597          0.674699  \n"
     ]
    }
   ],
   "source": [
    "# Turn the True/False values of our output to 1/0\n",
    "df_ml.lack_of_run_support = df_ml.lack_of_run_support.apply(lambda x: 1 if x==True else 0)\n",
    "\n",
    "print(df_ml.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the Data\n",
    "* 80% for training\n",
    "* 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64285714 0.17391304 0.32247557 0.45736434 0.35742972]\n",
      " [0.5        0.39130435 0.247557   0.34883721 0.42168675]\n",
      " [0.45238095 0.52173913 0.44299674 0.31782946 0.62650602]\n",
      " [0.33333333 0.39130435 0.42345277 0.25581395 0.59036145]]\n"
     ]
    }
   ],
   "source": [
    "# Turn the independent variables into an array to allow splitting\n",
    "indep_vars = df_ml[['home_run', 'p_unearned_run', 'k_percent', 'barrel_batted_rate', 'hard_hit_percent']].to_numpy()\n",
    "\n",
    "print(indep_vars[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the training and testing variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(indep_vars, df_ml['lack_of_run_support'], test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 799 X_test: 200\n",
      "y_train: 799 y_test: 200\n"
     ]
    }
   ],
   "source": [
    "# Confirm the correct sizes\n",
    "print('X_train:', len(X_train), 'X_test:', len(X_test))\n",
    "print('y_train:', len(y_train), 'y_test:', len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train and Optimize Models\n",
    "A. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model\n",
    "logistic_regression_model = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to make predictions\n",
    "logistic_regression_predict = logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Precision: 0.0\n",
      "MSE: 0.21\n",
      "RMSE: 0.458257569495584\n",
      "MAE: 0.21\n",
      "Recall: 0.0\n",
      "R2: -0.2658227848101262\n"
     ]
    }
   ],
   "source": [
    "# Grade the test\n",
    "print('Accuracy:', accuracy_score(y_test, logistic_regression_predict))\n",
    "print('Precision:', precision_score(y_test, logistic_regression_predict, zero_division=0.0))\n",
    "print('MSE:', mean_squared_error(y_test, logistic_regression_predict))\n",
    "print('RMSE:', root_mean_squared_error(y_test, logistic_regression_predict))\n",
    "print('MAE:', mean_absolute_error(y_test, logistic_regression_predict))\n",
    "print('Recall:', recall_score(y_test, logistic_regression_predict))\n",
    "print('R2:', r2_score(y_test, logistic_regression_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "decision_tree_model = tree.DecisionTreeClassifier().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "decision_tree_predict = decision_tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65\n",
      "Precision: 0.24074074074074073\n",
      "MSE: 0.35\n",
      "RMSE: 0.5916079783099616\n",
      "MAE: 0.35\n",
      "Recall: 0.30952380952380953\n",
      "R2: -1.1097046413502105\n"
     ]
    }
   ],
   "source": [
    "# Grade the test\n",
    "print('Accuracy:', accuracy_score(y_test, decision_tree_predict))\n",
    "print('Precision:', precision_score(y_test, decision_tree_predict))\n",
    "print('MSE:', mean_squared_error(y_test, decision_tree_predict))\n",
    "print('RMSE:', root_mean_squared_error(y_test, decision_tree_predict))\n",
    "print('MAE:', mean_absolute_error(y_test, decision_tree_predict))\n",
    "print('Recall:', recall_score(y_test, decision_tree_predict))\n",
    "print('R2:', r2_score(y_test, decision_tree_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Random Forest\n",
    "* A Random Forest can have a varying number of depths we can work with\n",
    "  * We have to determine which is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth      Accuracy       Precision           MSE    RMSE                MAE    Recall              R2                  \n",
      "1              0.79           0.0                 0.21   0.458257569495584   0.21   0.0                 -0.2658227848101262 \n",
      "2              0.79           0.0                 0.21   0.458257569495584   0.21   0.0                 -0.2658227848101262 \n",
      "3              0.79           0.0                 0.21   0.458257569495584   0.21   0.0                 -0.2658227848101262 \n",
      "4              0.8            1.0                 0.2    0.4472135954999579  0.2    0.047619047619047616-0.20554550934297744\n",
      "5              0.8            1.0                 0.2    0.4472135954999579  0.2    0.047619047619047616-0.20554550934297744\n",
      "6              0.8            1.0                 0.2    0.4472135954999579  0.2    0.047619047619047616-0.20554550934297744\n",
      "7              0.805          1.0                 0.195  0.44158804331639234 0.195  0.07142857142857142 -0.17540687160940305\n",
      "8              0.81           1.0                 0.19   0.43588989435406733 0.19   0.09523809523809523 -0.14526823387582866\n",
      "9              0.805          0.6666666666666666  0.195  0.44158804331639234 0.195  0.14285714285714285 -0.17540687160940305\n",
      "10             0.795          0.5714285714285714  0.205  0.4527692569068708  0.205  0.09523809523809523 -0.23568414707655183\n"
     ]
    }
   ],
   "source": [
    "# Print a header row\n",
    "print(f\"{'Max Depth':<15}{'Accuracy':<15}{'Precision':20}{'MSE':<7}{'RMSE':<20}{'MAE':7}{'Recall':<20}{'R2':<20}\")\n",
    "\n",
    "# Create a loop for testing different depths of the model\n",
    "for i in range(1,11):\n",
    "    # Create the model\n",
    "    random_forest_model = RandomForestClassifier(max_depth=i).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    random_forest_predict = random_forest_model.predict(X_test)\n",
    "\n",
    "    # Log outputs\n",
    "    print(f'{i:<15}{accuracy_score(y_test, random_forest_predict):<15}{precision_score(y_test, random_forest_predict, zero_division=0.0):<20}{mean_squared_error(y_test, random_forest_predict):<7}{root_mean_squared_error(y_test, random_forest_predict):<20}{mean_absolute_error(y_test, random_forest_predict):<7}{recall_score(y_test, random_forest_predict):<20}{r2_score(y_test, random_forest_predict):<20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is best?\n",
    "\n",
    "* 1-3 is the same\n",
    "* 4-6 is the same\n",
    "* 7 and 8 have a precision of 1.0\n",
    "  * may indicate overfitting\n",
    "* go with 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "naive_bayes_model = GaussianNB().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "naive_bayes_predict = naive_bayes_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Precision: 0.0\n",
      "MSE: 0.21\n",
      "RMSE: 0.458257569495584\n",
      "MAE: 0.21\n",
      "Recall: 0.0\n",
      "R2: -0.2658227848101262\n"
     ]
    }
   ],
   "source": [
    "# Grade the test\n",
    "print('Accuracy:', accuracy_score(y_test, naive_bayes_predict))\n",
    "print('Precision:', precision_score(y_test, naive_bayes_predict, zero_division=0.0))\n",
    "print('MSE:', mean_squared_error(y_test, naive_bayes_predict))\n",
    "print('RMSE:', root_mean_squared_error(y_test, naive_bayes_predict))\n",
    "print('MAE:', mean_absolute_error(y_test, naive_bayes_predict))\n",
    "print('Recall:', recall_score(y_test, naive_bayes_predict))\n",
    "print('R2:', r2_score(y_test, naive_bayes_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E. K-Nearest Neighbor\n",
    "* The number of neighbors can be modified\n",
    "  * Have to find the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neighbors    Accuracy       Precision           MSE    RMSE                MAE    Recall              R2                  \n",
      "3              0.76           0.40625             0.24   0.4898979485566356  0.24   0.30952380952380953 -0.446654611211573  \n",
      "5              0.785          0.46153846153846156 0.215  0.4636809247747852  0.215  0.14285714285714285 -0.29596142254370084\n",
      "7              0.765          0.2727272727272727  0.235  0.4847679857416329  0.235  0.07142857142857142 -0.4165159734779984 \n",
      "9              0.77           0.16666666666666666 0.23   0.47958315233127197 0.23   0.023809523809523808-0.386377335744424  \n",
      "11             0.795          0.5714285714285714  0.205  0.4527692569068708  0.205  0.09523809523809523 -0.23568414707655183\n",
      "13             0.785          0.4                 0.215  0.4636809247747852  0.215  0.047619047619047616-0.29596142254370084\n",
      "15             0.8            1.0                 0.2    0.4472135954999579  0.2    0.047619047619047616-0.20554550934297744\n"
     ]
    }
   ],
   "source": [
    "# Create initial header row\n",
    "print(f\"{'# Neighbors':<15}{'Accuracy':<15}{'Precision':20}{'MSE':<7}{'RMSE':<20}{'MAE':7}{'Recall':<20}{'R2':<20}\")\n",
    "\n",
    "\n",
    "# Create another loop\n",
    "for i in range(3, 16, 2):\n",
    "    # Create the model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    knn_predict = knn_model.predict(X_test)\n",
    "\n",
    "    # Log outputs\n",
    "    print(f'{i:<15}{accuracy_score(y_test, knn_predict):<15}{precision_score(y_test, knn_predict):<20}{mean_squared_error(y_test, knn_predict):<7}{root_mean_squared_error(y_test, knn_predict):<20}{mean_absolute_error(y_test, knn_predict):<7}{recall_score(y_test, knn_predict):<20}{r2_score(y_test, knn_predict):<20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is best?\n",
    "* 15 has the best numbers but possesses a perfect precision\n",
    "  * Possible overfitting\n",
    "* 11 has the next highest accuracy and precision\n",
    "  * Go with 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summarize\n",
    "\n",
    "| Algorithm                  | Accuracy | Precision | MSE   | RMSE  | MAE   | Recall | R2    |\n",
    "| :------------------------: | :------: | :-------: | :---: | :---: | :---: | :----: | :---: |\n",
    "| Logistic Regression        | 0.79     | 0.0       | 0.21  | 0.46  | 0.21  | 0.0    | -0.27 |\n",
    "| Decision Trees             | 0.71     | 0.30      | 0.295 | 0.54  | 0.295 | 0.31   | -0.78 |\n",
    "| Random Forest (9 branches) | 0.805    | 0.67      | 0.195 | 0.44  | 0.195 | 0.143  | -0.18 |\n",
    "| Naive Bayes                | 0.79     | 0.0       | 0.21  | 0.46  | 0.21  | 0.0    | -0.27 |\n",
    "| KNN (11 neighbors)         | 0.795    | 0.57      | 0.205 | 0.45  | 0.205 | 0.095  | -0.24 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Which is best?\n",
    "* Want to avoid precision of 0.0 or 1.0\n",
    "  * Remove (1) and (4)\n",
    "\n",
    "* Remaining order for each score (best to worst)\n",
    "  * Accuracy: (3), (5), (2)\n",
    "  * Precision: (3), (5), (2)\n",
    "  * MSE: (3), (5), (2)\n",
    "  * RMSE: (3), (5), (2)\n",
    "  * MAE: (3), (5), (2)\n",
    "  * Recall: (2), (3), (5)\n",
    "  * R2: (3), (5), (2)\n",
    "\n",
    "* Random Forest with 9 branches is the winner\n",
    "  * Use to predict what genreates the greatest lack of run support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".capstone_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
